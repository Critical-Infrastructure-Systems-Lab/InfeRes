#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jun 26 09:26:20 2025

@author: Hisham Eldardiry
"""

"""
area_postprocessing.py

This module provides functions to postprocess reservoir area time series 
generated by the InfeRes model or similar workflows. The postprocessing is 
applied to the full time series and includes two major steps:

1. **Outlier Removal**: Using statistical techniques (IQR filtering, LOWESS-based 
   detrending, and z-score thresholding), the module filters anomalous values 
   based on reservoir commissioning year (filling vs. operational periods).

2. **Smoothing and Interpolation**: After outlier removal, the cleaned time 
   series is interpolated and smoothed to produce consistent daily records.

"""


import pandas as pd
import numpy as np
from scipy import stats
from statsmodels.nonparametric.smoothers_lowess import lowess
from functools import reduce
from area_to_storage import convert_area_to_storage




def remove_outliers_filling_period(df: pd.DataFrame) -> pd.DataFrame:
    """
    Removes outliers from the reservoir filling period using z-score after LOWESS detrending.

    Parameters:
    - df: DataFrame with 'date' and 'level2_area_km2' columns.

    Returns:
    - DataFrame with outliers removed.
    """
    df = df.copy()
    df.rename(columns={'level2_area_km2': 'area'}, inplace=True)
    df.reset_index(drop=True, inplace=True)

    lowess_trend = lowess(df['area'], np.arange(len(df)), frac=0.1)
    df['Detrended'] = df['area'] - lowess_trend[:, 1]
    df['Z-Score'] = np.abs(stats.zscore(df['Detrended']))

    filtered = df[df['Z-Score'] <= 2.5].copy()
    if filtered.empty:
        filtered = df
    else:
        filtered = filtered.drop(columns=['Detrended', 'Z-Score'])

    return filtered.rename(columns={'area': 'level3_area_km2'})


def remove_outliers_operation_period(df: pd.DataFrame) -> pd.DataFrame:
    """
    Removes outliers from the operational period using IQR and z-score filtering.

    Parameters:
    - df: DataFrame with 'date' and 'level2_area_km2' columns.

    Returns:
    - DataFrame with outliers removed.
    """
    df = df.copy()
    df.rename(columns={'level2_area_km2': 'area'}, inplace=True)
    df.reset_index(drop=True, inplace=True)

    # First pass: IQR filtering
    Q1 = df['area'].quantile(0.25)
    Q3 = df['area'].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR
    df = df[(df['area'] >= lower_bound) & (df['area'] <= upper_bound)].copy()

    # Second pass: LOWESS + z-score
    df_tmp = df.copy()
    lowess_trend = lowess(df['area'], np.arange(len(df)), frac=0.1)
    df['Detrended'] = df['area'] - lowess_trend[:, 1]
    df['Z-Score'] = np.abs(stats.zscore(df['Detrended']))
    df = df[df['Z-Score'] <= 2]

    if df.empty:
        df = df_tmp
    else:
        df = df.drop(columns=['Detrended', 'Z-Score'])    

    return df.rename(columns={'area': 'level3_area_km2'})



def apply_outlier_filtering(data: pd.DataFrame, year_of_commission: int,sim_start_year: int) -> pd.DataFrame:
    """
    Main handler for outlier removal based on commissioning year.

    Parameters:
    - data: Raw time series data with 'level0_area_km2', 'date', and 'quality_flag' columns.
    - year_of_commission: Year the reservoir was commissioned.
    - sim_start_year: Start year of the simulation (e.g., 1985).

    Returns:
    - Cleaned DataFrame with outliers removed.
    """
    # Standardize date format
    # Ensure 'date' is datetime, skip format parsing if already correct
    if not np.issubdtype(data['date'].dtype, np.datetime64):
        if year_of_commission >= sim_start_year - 5:
            data['date'] = pd.to_datetime(data['date'], format='%d-%m-%y')
        else:
            data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')

    # Use only good-quality data if available
    if "quality_flag" in data.columns and len(data[data["quality_flag"] == 1]) != 0:
        data = data[data["quality_flag"] == 1]

    if year_of_commission >= sim_start_year-5:
        # Split data into pre- and post-commissioning for different outlier strategies
        data1 = data[data["date"] <= f'{year_of_commission}-12-31']
        data2 = data[data["date"] > f'{year_of_commission}-12-31']

        cleaned1 = remove_outliers_filling_period(data1)
        cleaned2 = remove_outliers_operation_period(data2)
        return pd.concat([cleaned1, cleaned2], ignore_index=True)
    else:
        return remove_outliers_operation_period(data)


def smooth_time_series(
    df: pd.DataFrame,
    sim_start_year: int,
    sim_end_year: int,
    variable: str = "storage",
    rolling_window: int = 15
) -> pd.DataFrame:
    """
    Interpolates and smooths a reservoir time series (e.g., storage, area, level)
    using combined values from Landsat and Sentinel sources.

    Parameters:
    - df: DataFrame containing columns: 'date', 'sensor', and a variable column
          in the form of f"{data_level}_{variable}_unit".
    - sim_start_year: Start year of the simulation.
    - sim_end_year: End year of the simulation.
    - variable: Reservoir variable to smooth ('storage', 'area', or 'elevation').
    - data_level: Prefix of the variable column indicating the processing level
                  (e.g., 'level1').
    - rolling_window: Size of the rolling window (in days) for smoothing.


    Returns:
    - DataFrame with columns ['date', '<variable>_<unit>'] where the variable is
      smoothed and interpolated.
    """
    df = df.copy()
    date_range = pd.date_range(
        start=f"{sim_start_year}-01-01",
        end=f"{sim_end_year}-12-31",
        freq='D'
    )
    date_df = pd.DataFrame({'date': date_range})

    # Determine column name based on variable and data level
    if variable == "storage":
        value_column = "level3_storage_mcm"
        output_column = "level4_storage_mcm"
    elif variable == "area":
        value_column = "level3_area_km2"
        output_column = "level4_area_km2"
    elif variable == "elevation":
        value_column = "level3_elevation_m"
        output_column = "level4_elevation_m"
    else:
        raise ValueError("variable must be one of: 'storage', 'area', or 'elevation'")

    # Filter and merge Landsat and Sentinel data
    df_landsat = df[df['sensor'] == 'landsat'][['date', value_column]].rename(columns={value_column: f"{variable}_landsat"})
    df_sentinel = df[df['sensor'] == 'sentinel'][['date', value_column]].rename(columns={value_column: f"{variable}_sentinel"})

    date_df = date_df.merge(df_landsat, on='date', how='left')
    date_df = date_df.merge(df_sentinel, on='date', how='left')

    # Compute average and apply interpolation + smoothing
    date_df['average'] = date_df[[f"{variable}_landsat", f"{variable}_sentinel"]].mean(axis=1, skipna=True)
    date_df['interpolated'] = date_df['average'].interpolate(method='linear')
    date_df[output_column] = date_df['interpolated'].rolling(window=rolling_window, min_periods=1).mean()

    return date_df[['date', output_column]]

def area_bias_correction(df: pd.DataFrame, res_max_area_km2: float, input_col: str = "level3_area_km2") -> pd.DataFrame:
    """
    Applies bias correction to estimated reservoir area time series using a known maximum area.

    Parameters:
    - df: DataFrame with 'date' and area column (e.g., 'level3_area_km2').
    - res_max_area_km2: Known maximum water surface area of the reservoir (from field data or literature).
    - input_col: Column name containing the estimated area values to be corrected.

    Returns:
    - DataFrame with a new column '<input_col>_bias_corrected' containing corrected area values.
    """
    df = df.copy()

    # Safety check
    if input_col not in df.columns:
        raise ValueError(f"Column '{input_col}' not found in DataFrame.")

    estimated_max_area = df[input_col].max()
    if estimated_max_area == 0 or np.isnan(estimated_max_area):
        raise ValueError("Estimated maximum area is zero or NaN. Bias correction cannot be applied.")

    correction_factor = res_max_area_km2 / estimated_max_area
    df[input_col] = df[input_col] * correction_factor
    
    return df
   
def generate_inferes_products(
    df_area: pd.DataFrame,
    curve_path: str,
    year_of_commission: int,
    sim_start_year: int,
    sim_end_year: int,
    res_max_area_km2: float,
    apply_bias_correction: bool = True,
    rolling_window: int = 15
) -> tuple[dict, pd.DataFrame]:
    """
    Processes raw InfeRes area time series through multiple stages of filtering and smoothing:
    
    Level 0 — Raw NDWI Area:
        Direct water surface area extracted from NDWI composite, before any filtering or enhancement.
    
    Level 1 — Cluster Filtered Area:
        Area derived after CLAHE normalization and KMeans clustering of NDWI values to isolate the dominant water body.
    
    Level 2 — Spatially Filtered Area:
        Area refined using zone-based filtering (based on water occurrence zones) and local window-based filtering
        to suppress noise and enforce spatial consistency in detection.
    
    Level 3 — Statistically Filtered Area (Outlier Removal):
        Post-processed area time series after temporal outlier removal using LOWESS detrending and z-score thresholding.
        Helps remove anomalies caused by clouds or algorithm artifacts.
    
    Level 4 — Smoothed Area:
        Daily interpolated and smoothed area time series using a rolling mean, producing a consistent and gap-filled
        product suitable for long-term analysis and volume estimation.
    
    res_max_area_km2 : float
       Known maximum surface area (in km²) of the reservoir, used to correct area bias via scaling.

    apply_bias_correction : bool, optional (default=True)
        Whether to apply bias correction using the known maximum surface area.

    rolling_window : int, optional (default=15)
        Rolling window size (in days) used for smoothing the time series in Level 4.


    Returns:
    - Dictionary of DataFrames for levels 0–4
    - Updated main area DataFrame with added columns:
      'post_outlier_removal_area_km2', 'post_smoothing_area_km2'
    """

    area_df = df_area.copy()
    area_df['date'] = pd.to_datetime(area_df['date'])


    # LEVEL 0: raw_area → compute but do not add to main CSV
    df0 = area_df.copy()
    df0=df0[["sensor","date","raw_area"]]
    df0.rename(columns={"raw_area": "level0_area_km2"}, inplace=True)
    area_series = df0["level0_area_km2"].reset_index(drop=True)
    elev_storage_0 = convert_area_to_storage(area_series, curve_path).round(2)
    df0 = df0.reset_index(drop=True)
    df0[["level0_elevation_m", "level0_storage_mcm"]] = elev_storage_0

    # LEVEL 1: pre_filtering_area_km2
    df1 = area_df.copy()
    df1=df1[["sensor","date","pre_filtering_area_km2"]]
    df1.rename(columns={"pre_filtering_area_km2": "level1_area_km2"}, inplace=True)
    area_series = df1["level1_area_km2"].reset_index(drop=True)
    elev_storage_1 = convert_area_to_storage(area_series, curve_path).round(2)
    df1 = df1.reset_index(drop=True)
    df1[["level1_elevation_m", "level1_storage_mcm"]] = elev_storage_1


    # LEVEL 2: post_filtering_area_km2
    df2 = area_df.copy()
    df2=df2[["sensor","date","post_filtering_area_km2"]]
    df2.rename(columns={"post_filtering_area_km2": "level2_area_km2"}, inplace=True)
    area_series = df2["level2_area_km2"].reset_index(drop=True)
    elev_storage_2 = convert_area_to_storage(area_series, curve_path).round(2)
    df2 = df2.reset_index(drop=True)
    df2[["level2_elevation_m", "level2_storage_mcm"]] = elev_storage_2


    # LEVEL 3: outlier removal (applied on post-filtering area)
    df3 = apply_outlier_filtering(df2, year_of_commission, sim_start_year)
    df3 = df3[["sensor","date", "level3_area_km2"]]  # only keep relevant
    df3['date'] = pd.to_datetime(df3['date'])
    area_df = area_df.merge(df3, on=["sensor", "date"], how="left")
    area_df.rename(columns={"level3_area_km2": "post_outlier_removal_area_km2"}, inplace=True)
    area_series = df3["level3_area_km2"].reset_index(drop=True)
    elev_storage_3 = convert_area_to_storage(area_series, curve_path).round(2)
    df3 = df3.reset_index(drop=True)
    df3[["level3_elevation_m", "level3_storage_mcm"]] = elev_storage_3


    # LEVEL 4: smoothing area, elevation, and storage independently
    # Apply bias correction
    if apply_bias_correction:
        df3 = area_bias_correction(df3, res_max_area_km2, input_col="level3_area_km2")
        
    level4_dfs = []
    
    for var in ['area', 'elevation', 'storage']:
        smoothed_df = smooth_time_series(
            df=df3,
            sim_start_year=sim_start_year,
            sim_end_year=sim_end_year,
            variable=var,
            rolling_window=rolling_window
        )
        smoothed_df['date'] = pd.to_datetime(smoothed_df['date'])
        
        level4_dfs.append(smoothed_df)
    
    # Merge all smoothed variables into single df4
    df4 = reduce(lambda left, right: pd.merge(left, right, on="date", how="outer"), level4_dfs)
    df4.dropna(subset=["level4_area_km2", "level4_elevation_m", "level4_storage_mcm"], inplace=True)
    df4=df4.reset_index(drop=True)


    # Return all levels and updated area_df
    return {
        "level0": df0,
        "level1": df1,
        "level2": df2,
        "level3": df3,
        "level4": df4
    }, area_df
